{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3242cb7e",
   "metadata": {},
   "source": [
    "# Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279258d",
   "metadata": {},
   "source": [
    "### Full Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36557bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = cv2.imread('../DATA/sammy.jpg')\n",
    "full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e356b72",
   "metadata": {},
   "source": [
    "### Template Image\n",
    "\n",
    "A subset of the image. Note how its actually the exact image. Later on we'll discuss more advanced methods for general matching, such as facial recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd180020",
   "metadata": {},
   "outputs": [],
   "source": [
    "face= cv2.imread('../DATA/sammy_face.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6167fb",
   "metadata": {},
   "source": [
    "# Template Matching Methods\n",
    "\n",
    "Make sure to watch the video for an explanation of the different methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dccb48",
   "metadata": {},
   "source": [
    "**Quick Note on **eval()** function in case you haven't seen it before!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773172e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = 'sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f764c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc = eval(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e101d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459a52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ef93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width,channels = face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95232bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Full Image to Search\n",
    "full = cv2.imread('../DATA/sammy.jpg')\n",
    "full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# The Template to Match\n",
    "face= cv2.imread('../DATA/sammy_face.jpg')\n",
    "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# All the 6 methods for comparison in a list\n",
    "# Note how we are using strings, later on we'll use the eval() function to convert to function\n",
    "methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR','cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b084d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in methods:\n",
    "    \n",
    "    # Create a copy of the image\n",
    "    full_copy = full.copy()\n",
    "    \n",
    "    # Get the actual function instead of the string\n",
    "    method = eval(m)\n",
    "\n",
    "    # Apply template Matching with the method\n",
    "    res = cv2.matchTemplate(full_copy,face,method)\n",
    "    \n",
    "    # Grab the Max and Min values, plus their locations\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    \n",
    "    # Set up drawing of Rectangle\n",
    "    \n",
    "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    # Notice the coloring on the last 2 left hand side images.\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc    \n",
    "    else:\n",
    "        top_left = max_loc\n",
    "        \n",
    "    # Assign the Bottom Right of the rectangle\n",
    "    bottom_right = (top_left[0] + width, top_left[1] + height)\n",
    "\n",
    "    # Draw the Red Rectangle\n",
    "    cv2.rectangle(full_copy,top_left, bottom_right, 255, 10)\n",
    "\n",
    "    # Plot the Images\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(res)\n",
    "    plt.title('Result of Template Matching')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(full_copy)\n",
    "    plt.title('Detected Point')\n",
    "    plt.suptitle(m)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3217d80",
   "metadata": {},
   "source": [
    "# Corner Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e0265",
   "metadata": {},
   "source": [
    "### The Image Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess = cv2.imread('../DATA/flat_chessboard.png')\n",
    "flat_chess = cv2.cvtColor(flat_chess,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4304fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_flat_chess = cv2.cvtColor(flat_chess,cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray_flat_chess,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca04bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_chess = cv2.imread('../DATA/real_chessboard.jpg')\n",
    "real_chess = cv2.cvtColor(real_chess,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(real_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee455b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_real_chess = cv2.cvtColor(real_chess,cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray_real_chess,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177c1c5",
   "metadata": {},
   "source": [
    "# Harris Corner Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5630541",
   "metadata": {},
   "source": [
    "**cornerHarris Function**\n",
    "\n",
    "*  src Input single-channel 8-bit or floating-point image.\n",
    "*  dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same size as src .\n",
    "*  blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).\n",
    "*  ksize Aperture parameter for the Sobel operator.\n",
    "*  k Harris detector free parameter. See the formula in DocString\n",
    "*  borderType Pixel extrapolation method. See #BorderTypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Gray Scale Image to Float Values\n",
    "gray = np.float32(gray_flat_chess)\n",
    "\n",
    "# Corner Harris Detection\n",
    "dst = cv2.cornerHarris(src=gray,blockSize=2,ksize=3,k=0.04)\n",
    "\n",
    "# result is dilated for marking the corners, not important to actual corner detection\n",
    "# this is just so we can plot out the points on the image shown\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "flat_chess[dst>0.01*dst.max()]=[255,0,0]\n",
    "\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Gray Scale Image to Float Values\n",
    "gray = np.float32(gray_real_chess)\n",
    "\n",
    "# Corner Harris Detection\n",
    "dst = cv2.cornerHarris(src=gray,blockSize=2,ksize=3,k=0.04)\n",
    "\n",
    "# result is dilated for marking the corners, not important to actual corner detection\n",
    "# this is just so we can plot out the points on the image shown\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "real_chess[dst>0.01*dst.max()]=[255,0,0]\n",
    "\n",
    "plt.imshow(real_chess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae56aaf",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Shi-Tomasi Corner Detector & Good Features to Track Paper\n",
    "\n",
    "[Link to Paper from Video](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf)\n",
    "\n",
    "goodFeatureToTrack Function Parameters\n",
    "\n",
    "* image Input 8-bit or floating-point 32-bit, single-channel image.\n",
    "* corners Output vector of detected corners.\n",
    "* maxCorners Maximum number of corners to return. If there are more corners than are found,the strongest of them is returned. `maxCorners <= 0` implies that no limit on the maximum is set and all detected corners are returned.\n",
    "* qualityLevel Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure less than 15 are rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859021fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reset the images since we drew on them\n",
    "flat_chess = cv2.imread('../DATA/flat_chessboard.png')\n",
    "flat_chess = cv2.cvtColor(flat_chess,cv2.COLOR_BGR2RGB)\n",
    "gray_flat_chess = cv2.cvtColor(flat_chess,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b371ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = cv2.goodFeaturesToTrack(gray_flat_chess,5,0.01,10)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(flat_chess,(x,y),3,255,-1)\n",
    "\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = cv2.goodFeaturesToTrack(gray_flat_chess,64,0.01,10)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(flat_chess,(x,y),3,255,-1)\n",
    "\n",
    "plt.imshow(flat_chess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9dc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_chess = cv2.imread('../DATA/real_chessboard.jpg')\n",
    "real_chess = cv2.cvtColor(real_chess,cv2.COLOR_BGR2RGB)\n",
    "gray_real_chess = cv2.cvtColor(real_chess,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = cv2.goodFeaturesToTrack(gray_real_chess,80,0.01,10)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(real_chess,(x,y),3,255,-1)\n",
    "\n",
    "plt.imshow(real_chess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdc2ba",
   "metadata": {},
   "source": [
    "# Canny Edge Detection\n",
    "https://en.wikipedia.org/wiki/Canny_edge_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806656ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../DATA/sammy_face.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=img, threshold1=127, threshold2=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ed943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=img, threshold1=0, threshold2=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e838f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c40f67",
   "metadata": {},
   "source": [
    "## Choosing Thresholds\n",
    "\n",
    "https://stackoverflow.com/questions/25125670/best-value-for-threshold-in-canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77baebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median pixel value\n",
    "med_val = np.median(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower bound is either 0 or 70% of the median value, whicever is higher\n",
    "lower = int(max(0, 0.7* med_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b65c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper bound is either 255 or 30% above the median value, whichever is lower\n",
    "upper = int(min(255,1.3 * med_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=img, threshold1=lower , threshold2=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed51f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917ef9a",
   "metadata": {},
   "source": [
    "Sometimes it helps to blur the images first, so we don't pick up minor edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_img = cv2.blur(img,ksize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303361a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=blurred_img, threshold1=lower , threshold2=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb73ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c8ea2",
   "metadata": {},
   "source": [
    "Let's play around with these threshold values even further!\n",
    "Often you'll need to experiment in regards to your specific dataset and what your final goal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffe9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=blurred_img, threshold1=lower , threshold2=upper+50)\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0d7ed",
   "metadata": {},
   "source": [
    "# Grid Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess = cv2.imread('../DATA/flat_chessboard.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c418b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flat_chess,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "found, corners = cv2.findChessboardCorners(flat_chess,(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if found:\n",
    "    print('OpenCV was able to find the corners')\n",
    "else:\n",
    "    print(\"OpenCV did not find corners. Double check your patternSize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chess_copy = flat_chess.copy()\n",
    "cv2.drawChessboardCorners(flat_chess_copy, (7, 7), corners, found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flat_chess_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad458fa",
   "metadata": {},
   "source": [
    "-------\n",
    "# Circle Based Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56dec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = cv2.imread('../DATA/dot_grid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "found, corners = cv2.findCirclesGrid(dots, (10,10), cv2.CALIB_CB_SYMMETRIC_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_image_circles = dots.copy()\n",
    "cv2.drawChessboardCorners(dbg_image_circles, (10, 10), corners, found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dbg_image_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eac497",
   "metadata": {},
   "source": [
    "# Contour Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4a48c",
   "metadata": {},
   "source": [
    "## External vs Internal Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e626d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../DATA/internal_external.png',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81129a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467992ad",
   "metadata": {},
   "source": [
    "**findContours**\n",
    "\n",
    "function will return back contours in an image, and based on the RETR method called, you can get back external, internal, or both:\n",
    "\n",
    "* cv2.RETR_EXTERNAL:Only extracts external contours\n",
    "* cv2.RETR_CCOMP: Extracts both internal and external contours organized in a two-level hierarchy\n",
    "* cv2.RETR_TREE: Extracts both internal and external contours organized in a  tree graph\n",
    "* cv2.RETR_LIST: Extracts all contours without any internal/external relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837939ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, contours, hierarchy = cv2.findContours(img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1637ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw External Contours\n",
    "\n",
    "# Set up empty array\n",
    "external_contours = np.zeros(image.shape)\n",
    "\n",
    "# For every entry in contours\n",
    "for i in range(len(contours)):\n",
    "    \n",
    "    # last column in the array is -1 if an external contour (no contours inside of it)\n",
    "    if hierarchy[0][i][3] == -1:\n",
    "        \n",
    "        # We can now draw the external contours from the list of contours\n",
    "        cv2.drawContours(external_contours, contours, i, 255, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(external_contours,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty array to hold internal contours\n",
    "image_internal = np.zeros(image.shape)\n",
    "\n",
    "# Iterate through list of contour arrays\n",
    "for i in range(len(contours)):\n",
    "    # If third column value is NOT equal to -1 than its internal\n",
    "    if hierarchy[0][i][3] != -1:\n",
    "        \n",
    "        # Draw the Contour\n",
    "        cv2.drawContours(image_internal, contours, i, 255, -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_internal,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac216c",
   "metadata": {},
   "source": [
    "# Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img,cmap='gray'):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "reeses = cv2.imread('../DATA/reeses_puffs.png',0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ed4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reeses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23645900",
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals = cv2.imread('../DATA/many_cereals.jpg',0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cereals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76625a",
   "metadata": {},
   "source": [
    "--------\n",
    "# Brute Force Detection with ORB Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(reeses,None)\n",
    "kp2, des2 = orb.detectAndCompute(cereals,None)\n",
    "\n",
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 25 matches.\n",
    "reeses_matches = cv2.drawMatches(reeses,kp1,cereals,kp2,matches[:25],None,flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reeses_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae0c1b",
   "metadata": {},
   "source": [
    "# Brute-Force Matching with SIFT Descriptors and Ratio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SIFT Object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(reeses,None)\n",
    "kp2, des2 = sift.detectAndCompute(cereals,None)\n",
    "\n",
    "# BFMatcher with default params\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for match1,match2 in matches:\n",
    "    if match1.distance < 0.75*match2.distance:\n",
    "        good.append([match1])\n",
    "\n",
    "# cv2.drawMatchesKnn expects list of lists as matches.\n",
    "sift_matches = cv2.drawMatchesKnn(reeses,kp1,cereals,kp2,good,None,flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563daa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sift_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1c3c3",
   "metadata": {},
   "source": [
    "# FLANN based Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15707f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(reeses,None)\n",
    "kp2, des2 = sift.detectAndCompute(cereals,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)  \n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "good = []\n",
    "\n",
    "# ratio test\n",
    "for i,(match1,match2) in enumerate(matches):\n",
    "    if match1.distance < 0.7*match2.distance:\n",
    "        \n",
    "        good.append([match1])\n",
    "\n",
    "\n",
    "flann_matches = cv2.drawMatchesKnn(reeses,kp1,cereals,kp2,good,None,flags=0)\n",
    "\n",
    "display(flann_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(reeses,None)\n",
    "kp2, des2 = sift.detectAndCompute(cereals,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)  \n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test\n",
    "for i,(match1,match2) in enumerate(matches):\n",
    "    if match1.distance < 0.7*match2.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = 0)\n",
    "\n",
    "flann_matches = cv2.drawMatchesKnn(reeses,kp1,cereals,kp2,matches,None,**draw_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(flann_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f586191",
   "metadata": {},
   "source": [
    "# Image Segementation and the Watershed Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img,cmap=None):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img,cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17990329",
   "metadata": {},
   "source": [
    "## Our Task: Draw Contours Around the Coins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd2d35",
   "metadata": {},
   "source": [
    "## Common Coin Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d098f4a",
   "metadata": {},
   "source": [
    "## Naive Approach\n",
    "\n",
    "Let's try to simply use a threshold and then use findContours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b7c80",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_coins = cv2.imread('../DATA/pennies.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cd43d",
   "metadata": {},
   "source": [
    "### Apply Median Blurring\n",
    "\n",
    "We have too much detail in this image, including light, the face edges on the coins, and too much detail in the background. Let's use Median Blur Filtering to blur the image a bit, which will be useful later on when we threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_blur = cv2.medianBlur(sep_coins,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be010538",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sep_coins = cv2.cvtColor(sep_blur,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(gray_sep_coins,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b1164",
   "metadata": {},
   "source": [
    "## Binary Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, sep_thresh = cv2.threshold(gray_sep_coins,160,255,cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_thresh,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1546642",
   "metadata": {},
   "source": [
    "## FindContours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, contours, hierarchy = cv2.findContours(sep_thresh.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16022e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every entry in contours\n",
    "for i in range(len(contours)):\n",
    "    \n",
    "    # last column in the array is -1 if an external contour (no contours inside of it)\n",
    "    if hierarchy[0][i][3] == -1:\n",
    "        \n",
    "        # We can now draw the external contours from the list of contours\n",
    "        cv2.drawContours(sep_coins, contours, i, (255, 0, 0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f74c5",
   "metadata": {},
   "source": [
    "# Watershed Algorithm\n",
    "\n",
    "Let's now try the watershed algorithm apporach to draw contours around the pennies. Also make sure to watch the video to understand what the function calls are doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484f535",
   "metadata": {},
   "source": [
    "## Using the WaterShed Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c929fba",
   "metadata": {},
   "source": [
    "#### Step 1: Read Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../DATA/pennies.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5abfa5",
   "metadata": {},
   "source": [
    "#### Step 2: Apply Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbde764",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.medianBlur(img,35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9595da1",
   "metadata": {},
   "source": [
    "#### Step 3: Convert to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc60445",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4fd37",
   "metadata": {},
   "source": [
    "#### Step 4: Apply Threshold (Inverse Binary with OTSU as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(thresh,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10d7b6",
   "metadata": {},
   "source": [
    "### Optional Step 5: Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b14195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(opening,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc213e",
   "metadata": {},
   "source": [
    "#### Step 6: Grab Background that you are sure of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening,kernel,iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sure_bg,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea7df5",
   "metadata": {},
   "source": [
    "#### Step 7: Find Sure Foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc30c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dist_transform,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sure_fg,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a50742",
   "metadata": {},
   "source": [
    "#### Step 8: Find Unknown Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27165e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unknown,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66b16f",
   "metadata": {},
   "source": [
    "#### Step 9: Label Markers of Sure Foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers+1\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown==255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(markers,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ad9e3",
   "metadata": {},
   "source": [
    "#### Step 10: Apply Watershed Algorithm to find Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = cv2.watershed(img,markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fba96",
   "metadata": {},
   "source": [
    "#### Step 11: Find Contours on Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, contours, hierarchy = cv2.findContours(markers.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# For every entry in contours\n",
    "for i in range(len(contours)):\n",
    "    \n",
    "    # last column in the array is -1 if an external contour (no contours inside of it)\n",
    "    if hierarchy[0][i][3] == -1:\n",
    "        \n",
    "        # We can now draw the external contours from the list of contours\n",
    "        cv2.drawContours(sep_coins, contours, i, (255, 0, 0), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sep_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819b993",
   "metadata": {},
   "source": [
    "# Custom Seeds with the WaterShed Algorithm\n",
    "\n",
    "Previously we did a lot of work for OpenCV to set Markers to provide seeds to the Watershed Algorithm. But what if we just provide seeds ourselves? Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be3ee4",
   "metadata": {},
   "source": [
    "### Read in the Image and Make a Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d035858",
   "metadata": {},
   "outputs": [],
   "source": [
    "road = cv2.imread('../DATA/road_image.jpg')\n",
    "road_copy = np.copy(road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f513e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cd5fb",
   "metadata": {},
   "source": [
    "#### Create an empty space for the results to be drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "road.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "road.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_image = np.zeros(road.shape[:2],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = np.zeros(road.shape,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82da7d3",
   "metadata": {},
   "source": [
    "### Create colors for Markers\n",
    "\n",
    "https://matplotlib.org/examples/color/colormaps_reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f15050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992d874",
   "metadata": {},
   "source": [
    "Returns (R,G,B,Alpha) we only need RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b128827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.tab10(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.tab10(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a65028",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cm.tab10(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cm.tab10(0))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803aa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cm.tab10(0))[:3]*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(cm.tab10(0))[:3]*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a63571",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(x.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f934a",
   "metadata": {},
   "source": [
    "Let's make a function for all those steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f68503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb(i):\n",
    "    x = np.array(cm.tab10(i))[:3]*255\n",
    "    return tuple(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4eb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One color for each single digit\n",
    "for i in range(10):\n",
    "    colors.append(create_rgb(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081d1ec",
   "metadata": {},
   "source": [
    "### Setting Up Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ad0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddae21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers 0-9\n",
    "n_markers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "current_marker = 1\n",
    "marks_updated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981033fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global marks_updated \n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        \n",
    "        # TRACKING FOR MARKERS\n",
    "        cv2.circle(marker_image, (x, y), 10, (current_marker), -1)\n",
    "        \n",
    "        # DISPLAY ON USER IMAGE\n",
    "        cv2.circle(road_copy, (x, y), 10, colors[current_marker], -1)\n",
    "        marks_updated = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('Road Image')\n",
    "cv2.setMouseCallback('Road Image', mouse_callback)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # SHow the 2 windows\n",
    "    cv2.imshow('WaterShed Segments', segments)\n",
    "    cv2.imshow('Road Image', road_copy)\n",
    "        \n",
    "        \n",
    "    # Close everything if Esc is pressed\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "        \n",
    "    # Clear all colors and start over if 'c' is pressed\n",
    "    elif k == ord('c'):\n",
    "        road_copy = road.copy()\n",
    "        marker_image = np.zeros(road.shape[0:2], dtype=np.int32)\n",
    "        segments = np.zeros(road.shape,dtype=np.uint8)\n",
    "        \n",
    "    # If a number 0-9 is chosen index the color\n",
    "    elif k > 0 and chr(k).isdigit():\n",
    "        # chr converts to printable digit\n",
    "        \n",
    "        current_marker  = int(chr(k))\n",
    "        \n",
    "        # CODE TO CHECK INCASE USER IS CARELESS\n",
    "#         n = int(chr(k))\n",
    "#         if 1 <= n <= n_markers:\n",
    "#             current_marker = n\n",
    "    \n",
    "    # If we clicked somewhere, call the watershed algorithm on our chosen markers\n",
    "    if marks_updated:\n",
    "        \n",
    "        marker_image_copy = marker_image.copy()\n",
    "        cv2.watershed(road, marker_image_copy)\n",
    "        \n",
    "        segments = np.zeros(road.shape,dtype=np.uint8)\n",
    "        \n",
    "        for color_ind in range(n_markers):\n",
    "            segments[marker_image_copy == (color_ind)] = colors[color_ind]\n",
    "        \n",
    "        marks_updated = False\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19481ff7",
   "metadata": {},
   "source": [
    "# Face Detection with Haar Cascades\n",
    "\n",
    "**Note: This is face *detection* NOT face *recognition*. We are only detecting if a face is in an image, not who the face actually is. That requires deep learning which we'll go over later!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8c79d",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nadia = cv2.imread('../DATA/Nadia_Murad.jpg',0)\n",
    "denis = cv2.imread('../DATA/Denis_Mukwege.jpg',0)\n",
    "solvay = cv2.imread('../DATA/solvay_conference.jpg',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(nadia,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denis,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(solvay,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e290bd",
   "metadata": {},
   "source": [
    "## Cascade Files\n",
    "\n",
    "OpenCV comes with these pre-trained cascade files, we've relocated the .xml files for you in our own DATA folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66cd39d",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('../DATA/haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    \n",
    "  \n",
    "    face_img = img.copy()\n",
    "  \n",
    "    face_rects = face_cascade.detectMultiScale(face_img) \n",
    "    \n",
    "    for (x,y,w,h) in face_rects: \n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 10) \n",
    "        \n",
    "    return face_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect_face(denis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect_face(nadia)\n",
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d466c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets errors!\n",
    "result = detect_face(solvay)\n",
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab641108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_detect_face(img):\n",
    "    \n",
    "    face_img = img.copy()\n",
    "  \n",
    "    face_rects = face_cascade.detectMultiScale(face_img,scaleFactor=1.2, minNeighbors=5) \n",
    "    \n",
    "    for (x,y,w,h) in face_rects: \n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 10) \n",
    "        \n",
    "    return face_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't detect the side face.\n",
    "result = adj_detect_face(solvay)\n",
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfef356",
   "metadata": {},
   "source": [
    "## Eye Cascade File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier('../DATA/haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_eyes(img):\n",
    "    \n",
    "    face_img = img.copy()\n",
    "  \n",
    "    eyes = eye_cascade.detectMultiScale(face_img) \n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in eyes: \n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 10) \n",
    "        \n",
    "    return face_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detect_eyes(nadia)\n",
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes = eye_cascade.detectMultiScale(denis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c171522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# White around the pupils is not distinct enough to detect Denis' eyes here!\n",
    "result = detect_eyes(denis)\n",
    "plt.imshow(result,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66e1e8",
   "metadata": {},
   "source": [
    "## Conjunction with Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while True: \n",
    "    \n",
    "    ret, frame = cap.read(0) \n",
    "     \n",
    "    frame = detect_face(frame)\n",
    " \n",
    "    cv2.imshow('Video Face Detection', frame) \n",
    " \n",
    "    c = cv2.waitKey(1) \n",
    "    if c == 27: \n",
    "        break \n",
    "        \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185268b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0e6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649ea7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98952017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d72d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f2e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
